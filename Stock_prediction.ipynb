{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock-prediction\n",
    "The full code of the Data-Exploration Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.25,rc={'figure.figsize':(14,12)})\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(314)\n",
    "tf.random.set_seed(314)\n",
    "random.seed(314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle two arrays in the same way\n",
    "def shuffle_in_unison(a, b):\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "'''\n",
    "Loads the Data from Yahoo finance and prepares it for later use\n",
    "<params>\n",
    "    ticker: ticker names from yahoo finance / Pandas Dataframe for saved data\n",
    "    n_steps: sequence size\n",
    "    lookup_step: how far into the future should be predicted\n",
    "    test_size: train/test split in percent\n",
    "    feature_columns: list of columns to make sure that everything exists\n",
    "return: result Dataframe\n",
    "'''\n",
    "def load_data(ticker, n_steps=50, lookup_step=1, test_size=0.2, \n",
    "              feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \n",
    "    # is it a ticker or data loaded from csv\n",
    "    if isinstance(ticker, str):\n",
    "        # load new data\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # csv\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "    \n",
    "    # result to be returned\n",
    "    result = {}\n",
    "    result['df'] = df.copy()\n",
    "    \n",
    "    # making sure that all columns exist\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "        \n",
    "    # add date\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "    \n",
    "    # scales data from 0 to 1\n",
    "    column_scaler = {}\n",
    "    for column in feature_columns:\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "        column_scaler[column] = scaler\n",
    "    # add the MinMaxScaler instances to the result returned\n",
    "    result[\"column_scaler\"] = column_scaler\n",
    "    \n",
    "    # adds the label as a column by shifting with the lookup step\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    \n",
    "    # last labels contain NaNs \n",
    "    # shouldnt be dropped\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    \n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # build the sequences \n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    \n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    \n",
    "    result['last_sequence'] = last_sequence\n",
    "    \n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "        \n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # split the dataset randomly\n",
    "    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, test_size=test_size, shuffle=True)\n",
    "    \n",
    "    # shuffle the datasets for training\n",
    "    shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "    shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Builds the neural Network model\n",
    "<params>\n",
    "    sequence_length: the length of one sequence\n",
    "    n_features: the number of feature columns that is inserted as a input\n",
    "    units: number of neurons per layer\n",
    "    cell: specifies the kind of layer added (in this case we are using LSTM)\n",
    "    n_layers: number of layers\n",
    "    dropout: the dropout rate that is applied to the input (in percent)\n",
    "    loss: the loss-funktion that is used\n",
    "    optimizer: the optimizer that is used\n",
    "return: build model\n",
    "'''\n",
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"huber_loss\", optimizer=\"adam\"):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plots the predicted price alongside the true price\n",
    "<params>\n",
    "    test_df: in this case the final Dataframe from get_final_df()\n",
    "''' \n",
    "def plot_graph(test_df):\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "creates the final Dataframe with the features, the true prices and the predicted prices\n",
    "<params>\n",
    "    model: the model that was loaded with the trained weigths\n",
    "    data: the data Dataframe from load_data()\n",
    "returns: the final Dataframe\n",
    "'''\n",
    "def get_final_df(model, data):\n",
    "    # if predicted future price is higher than the current\n",
    "    buy_profit  = lambda current, pred_future, true_future: true_future - current if pred_future > current else 0\n",
    "    \n",
    "    # if the predicted future price is lower than the current price\n",
    "    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future < current else 0\n",
    "    \n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    \n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)    \n",
    "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    \n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    \n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    \n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    \n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, final_df[\"adjclose\"], final_df[f\"adjclose_{LOOKUP_STEP}\"], final_df[f\"true_adjclose_{LOOKUP_STEP}\"]))\n",
    "    \n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, final_df[\"adjclose\"], final_df[f\"adjclose_{LOOKUP_STEP}\"], final_df[f\"true_adjclose_{LOOKUP_STEP}\"]))\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "gives the predicted future price\n",
    "<params>\n",
    "    model: the model that was loaded with the trained weigths\n",
    "    data: the data Dataframe from load_data()\n",
    "return: predicted future price\n",
    "'''\n",
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    \n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    \n",
    "    # get the prediction \n",
    "    prediction = model.predict(last_sequence)\n",
    "    \n",
    "    # invert scaling\n",
    "    predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    \n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# sequence length\n",
    "N_STEPS = 50\n",
    "\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 15\n",
    "\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "\n",
    "### model parameters\n",
    "N_LAYERS = 2\n",
    "\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "\n",
    "# number of LSTM neurons\n",
    "UNITS = 256\n",
    "\n",
    "# applies Dropout to the input\n",
    "DROPOUT = 0.4\n",
    "\n",
    "# training parameters\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "# which stock to use as ticker\n",
    "ticker = \"TMV.DE\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}.csv\")\n",
    "\n",
    "# model name based on params. used as filename for the saved weights \n",
    "model_name = f\"{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"model_weigths\"):\n",
    "    os.mkdir(\"model_weigths\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0386 - mean_absolute_error: 0.2089\n",
      "Epoch 1: val_loss improved from inf to 0.00273, saving model to model_weigths\\TMV.DE-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "9/9 [==============================] - 13s 636ms/step - loss: 0.0386 - mean_absolute_error: 0.2089 - val_loss: 0.0027 - val_mean_absolute_error: 0.0655\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0115 - mean_absolute_error: 0.1185\n",
      "Epoch 2: val_loss improved from 0.00273 to 0.00081, saving model to model_weigths\\TMV.DE-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "9/9 [==============================] - 5s 502ms/step - loss: 0.0115 - mean_absolute_error: 0.1185 - val_loss: 8.0612e-04 - val_mean_absolute_error: 0.0312\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0077 - mean_absolute_error: 0.0964\n",
      "Epoch 3: val_loss did not improve from 0.00081\n",
      "9/9 [==============================] - 4s 459ms/step - loss: 0.0077 - mean_absolute_error: 0.0964 - val_loss: 0.0014 - val_mean_absolute_error: 0.0434\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0071 - mean_absolute_error: 0.0909\n",
      "Epoch 4: val_loss did not improve from 0.00081\n",
      "9/9 [==============================] - 4s 464ms/step - loss: 0.0071 - mean_absolute_error: 0.0909 - val_loss: 9.0969e-04 - val_mean_absolute_error: 0.0334\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.0075 - mean_absolute_error: 0.0949\n",
      "Epoch 5: val_loss did not improve from 0.00081\n",
      "9/9 [==============================] - 4s 445ms/step - loss: 0.0075 - mean_absolute_error: 0.0949 - val_loss: 8.7286e-04 - val_mean_absolute_error: 0.0326\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER)\n",
    "\n",
    "# ModelCheckpoint saves the best model\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"model_weigths\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "# tensorboard to log changes\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "\n",
    "# train the model\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"], batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMV.DE-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256\n"
     ]
    }
   ],
   "source": [
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"model_weigths\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_data = pd.read_csv(ticker_data_filename)\n",
    "data = load_data(stored_data, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "\n",
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "\n",
    "# calculate the mean absolute error\n",
    "mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 87ms/step\n"
     ]
    }
   ],
   "source": [
    "# get the final dataframe for the testing set\n",
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    }
   ],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy as percentage of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 15 days is 10.79$\n",
      "huber_loss loss: 0.0008061228436417878\n",
      "Mean Absolute Error: 10.50468309397823\n",
      "Accuracy score: 0.5271317829457365\n",
      "Total buy profit: -11.953002929687507\n",
      "Total sell profit: 8.819996833801264\n",
      "Total profit: -3.133006095886243\n",
      "Profit per trade: -0.024286868960358473\n"
     ]
    }
   ],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-596775213e1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# plot true/pred prices graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplot_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_graph' is not defined"
     ]
    }
   ],
   "source": [
    "# plot true/pred prices graph\n",
    "plot_graph(final_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b99f0b8c6d578cc3af401c94438b80c11e140ab87d68cb3e1b5b982b15eaef2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
